<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OpenCV | Hugo Academic CV Theme</title>
    <link>http://localhost:1313/tags/opencv/</link>
      <atom:link href="http://localhost:1313/tags/opencv/index.xml" rel="self" type="application/rss+xml" />
    <description>OpenCV</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 07 Nov 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>OpenCV</title>
      <link>http://localhost:1313/tags/opencv/</link>
    </image>
    
    <item>
      <title>Sparse 3D Reconstruction and Bundle Adjustment</title>
      <link>http://localhost:1313/project/sparse-reconstruction/</link>
      <pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/sparse-reconstruction/</guid>
      <description>&lt;p&gt;This project implements a full Structure from Motion (SfM) pipeline on a &lt;strong&gt;Buddha statue&lt;/strong&gt; using a sequence of &lt;strong&gt;24 grayscale images&lt;/strong&gt;. It combines feature detection, epipolar geometry, camera pose recovery, triangulation, and bundle adjustment.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;24 images of a wooden Buddha statue captured at different angles&lt;/li&gt;
&lt;li&gt;Enhanced using &lt;strong&gt;CLAHE (Contrast Limited Adaptive Histogram Equalization)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Features extracted using &lt;strong&gt;SIFT&lt;/strong&gt; with custom parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pipeline-overview&#34;&gt;Pipeline Overview&lt;/h3&gt;
&lt;h4 id=&#34;1-image-preprocessing&#34;&gt;1. Image Preprocessing&lt;/h4&gt;
&lt;p&gt;Using CLAHE improves contrast on low-texture surfaces like carved wood.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;img src=&#34;processed.png&#34; width=&#34;1000&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-sift-feature-detection&#34;&gt;2. SIFT Feature Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Applied to all 24 images&lt;/li&gt;
&lt;li&gt;Used &lt;strong&gt;BFMatcher&lt;/strong&gt; with ratio test&lt;/li&gt;
&lt;li&gt;Matches filtered via &lt;strong&gt;RANSAC&lt;/strong&gt; for outlier rejection&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;img src=&#34;features.png&#34; width=&#34;1000&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-essential-matrix--pose-recovery&#34;&gt;3. Essential Matrix &amp;amp; Pose Recovery&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Computed Essential matrix using calibrated camera matrix&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;cv2.recoverPose()&lt;/code&gt; to derive relative rotation and translation between views&lt;/li&gt;
&lt;li&gt;Built a chain of camera poses from image 0 onward&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-triangulation&#34;&gt;4. Triangulation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;3D points computed from pixel correspondences using &lt;code&gt;cv2.triangulatePoints()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All 3D points stored in homogeneous form&lt;/li&gt;
&lt;li&gt;Colored and visualized using Plotly&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;5-bundle-adjustment-with-gtsam&#34;&gt;5. Bundle Adjustment with GTSAM&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built a factor graph with:
&lt;ul&gt;
&lt;li&gt;Camera pose priors&lt;/li&gt;
&lt;li&gt;Between factors from pose transitions&lt;/li&gt;
&lt;li&gt;Projection factors from 2D-3D matches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;Levenberg-MarquardtOptimizer&lt;/code&gt; for refinement&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;h4 id=&#34;initial-3d-trajectory&#34;&gt;Initial 3D Trajectory&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Initial Trajectory&#34; srcset=&#34;
               /project/sparse-reconstruction/Initial_plot_hu12090084634136375331.webp 400w,
               /project/sparse-reconstruction/Initial_plot_hu6971695684859321629.webp 760w,
               /project/sparse-reconstruction/Initial_plot_hu11908849571792030370.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/sparse-reconstruction/Initial_plot_hu12090084634136375331.webp&#34;
               width=&#34;760&#34;
               height=&#34;282&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;optimized-3d-trajectory-after-bundle-adjustment&#34;&gt;Optimized 3D Trajectory after Bundle Adjustment&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Optimized Trajectory&#34; srcset=&#34;
               /project/sparse-reconstruction/Optimized_plot_hu1365288965832748826.webp 400w,
               /project/sparse-reconstruction/Optimized_plot_hu5432368712792554619.webp 760w,
               /project/sparse-reconstruction/Optimized_plot_hu6617160073620067270.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/sparse-reconstruction/Optimized_plot_hu1365288965832748826.webp&#34;
               width=&#34;760&#34;
               height=&#34;282&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average reprojection error reduced by ~15%&lt;/li&gt;
&lt;li&gt;Landmark cloud tightened around object geometry&lt;/li&gt;
&lt;li&gt;Rotation drift corrected with global optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;tools--libraries&#34;&gt;Tools &amp;amp; Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenCV (SIFT, RANSAC, triangulation)&lt;/li&gt;
&lt;li&gt;NumPy, Matplotlib, Plotly&lt;/li&gt;
&lt;li&gt;GTSAM (factor graph + BA)&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/Sparse-3D-Reconstruction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project demonstrates a scalable pipeline for SfM using minimal dependencies. It serves as a foundation for integrating real-time VIO or stereo SLAM on embedded platforms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Photo Mosaicking of Low-Contrast Underwater Images</title>
      <link>http://localhost:1313/project/photo-mosaicking/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/photo-mosaicking/</guid>
      <description>&lt;p&gt;This project implements a full photo mosaicking and optimization pipeline using low-contrast underwater images from the &lt;strong&gt;Skerki Bank Roman shipwreck&lt;/strong&gt; dataset. The approach registers both sequential and non-sequential images using SIFT and RANSAC, computes affine transformations, and optimizes a global trajectory using GTSAM.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pipeline-breakdown&#34;&gt;Pipeline Breakdown&lt;/h3&gt;
&lt;h4 id=&#34;1-clahe-image-enhancement&#34;&gt;1Ô∏è‚É£ CLAHE Image Enhancement&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Applies histogram equalization to improve contrast and enhance keypoints.&lt;/li&gt;
&lt;li&gt;OpenCV CLAHE was used on each grayscale frame.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clahe&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;createCLAHE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clipLimit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tileGridSize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clahe_image&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clahe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gray_image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/clahe_grid.png&#34; alt=&#34;CLAHE Enhanced Images&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-sift-feature-detection&#34;&gt;2Ô∏è‚É£ SIFT Feature Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Detected keypoints using tuned SIFT settings:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nfeatures=5000&lt;/code&gt;, &lt;code&gt;contrastThreshold=0.025&lt;/code&gt;, &lt;code&gt;nOctaveLayers=8&lt;/code&gt;, &lt;code&gt;sigma=1.5&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sift&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SIFT_create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;kp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sift&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;detectAndCompute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/features_grid.png&#34; alt=&#34;Detected Keypoints&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-feature-matching--ransac-filtering&#34;&gt;3Ô∏è‚É£ Feature Matching + RANSAC Filtering&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Matched descriptors using Brute-Force Matcher + Lowe‚Äôs ratio test.&lt;/li&gt;
&lt;li&gt;Applied &lt;code&gt;cv2.estimateAffine2D&lt;/code&gt; with RANSAC to compute and refine transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;knnMatch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;des1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;des2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;good&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.75&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;estimateAffine2D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;method&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RANSAC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/affine_grid.png&#34; alt=&#34;Affine Transformation Filtering&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-pose-graph-construction-gtsam&#34;&gt;4Ô∏è‚É£ Pose Graph Construction (GTSAM)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built a factor graph using all non-repeating image pairs.&lt;/li&gt;
&lt;li&gt;Relative poses (affine transforms) were added as edges.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BetweenFactorPose2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T_ij&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;noise_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;initial-trajectoryplot_beforepng&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Initial Trajectory&#34; srcset=&#34;
               /project/photo-mosaicking/plot_before_hu6875028771095386571.webp 400w,
               /project/photo-mosaicking/plot_before_hu3546410195309766098.webp 760w,
               /project/photo-mosaicking/plot_before_hu3825931163024726418.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/photo-mosaicking/plot_before_hu6875028771095386571.webp&#34;
               width=&#34;571&#34;
               height=&#34;455&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/h2&gt;
&lt;h4 id=&#34;5-global-bundle-adjustment&#34;&gt;5Ô∏è‚É£ Global Bundle Adjustment&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Used GTSAM‚Äôs Levenberg-Marquardt optimizer to refine global poses.&lt;/li&gt;
&lt;li&gt;Corrects drift and adjusts poses to minimize total residual error.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gtsam&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LevenbergMarquardtOptimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_estimate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Optimized Trajectory&#34; srcset=&#34;
               /project/photo-mosaicking/plot_after_hu16779151673749428898.webp 400w,
               /project/photo-mosaicking/plot_after_hu14280678253068865284.webp 760w,
               /project/photo-mosaicking/plot_after_hu13790384701387224375.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/photo-mosaicking/plot_after_hu16779151673749428898.webp&#34;
               width=&#34;580&#34;
               height=&#34;455&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;techniques-used&#34;&gt;Techniques Used&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Image normalization + CLAHE&lt;/li&gt;
&lt;li&gt;SIFT feature detection and matching&lt;/li&gt;
&lt;li&gt;RANSAC for outlier rejection&lt;/li&gt;
&lt;li&gt;Homography estimation using Levenberg‚ÄìMarquardt&lt;/li&gt;
&lt;li&gt;Graph construction (GTSAM)&lt;/li&gt;
&lt;li&gt;Loop closure detection&lt;/li&gt;
&lt;li&gt;Pose optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Successfully registered both sequential and non-sequential image pairs&lt;/li&gt;
&lt;li&gt;Constructed optimized pose graphs for 6 and 29 image subsets&lt;/li&gt;
&lt;li&gt;Achieved a ~20% improvement in alignment after bundle adjustment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-related-files&#34;&gt;üìÅ Related Files&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/photo-mosaicking-skerki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìÅ &lt;a href=&#34;https://drive.google.com/drive/folders/1AtvT65txGIgAG23NRs3EkvDET036a81O&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Skerki Dataset Reference (Google Drive)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìì &lt;a href=&#34;http://localhost:1313/files/Part1_and_2.ipynb&#34;&gt;Project Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìì &lt;a href=&#34;http://localhost:1313/files/Part_3.ipynb&#34;&gt;Extended Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-references&#34;&gt;üìñ References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pizarro &amp;amp; Singh (2003): &lt;em&gt;Toward large-area mosaicing for underwater scientific applications.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ballard et al. (1998, 2000): &lt;em&gt;Roman shipwreck discovery using submersible tech.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Real-Time 2D Object Recognition with Feature Matching</title>
      <link>http://localhost:1313/project/object-recognition/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/object-recognition/</guid>
      <description>&lt;p&gt;This project developed a &lt;strong&gt;real-time webcam-based system&lt;/strong&gt; to detect and classify 2D objects using image segmentation, shape features, and distance-based classifiers. The system supports capturing and labeling training data interactively and recognizes both scale- and rotation-invariant objects.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-workflow&#34;&gt;üß† Workflow&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Thresholding Input Frames&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic thresholding via simplified K-means clustering&lt;/li&gt;
&lt;li&gt;Binary image separates object from background&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Morphological Filtering&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used dilation and erosion to clean noise and fill holes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Connected Components &amp;amp; Region Segmentation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identified distinct objects&lt;/li&gt;
&lt;li&gt;Filtered out small noisy components&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Centroids, central moments, bounding box, percent fill&lt;/li&gt;
&lt;li&gt;Orientation angle from least central moment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Interface&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;&#39;n&#39;&lt;/code&gt; to label new objects and save features to CSV&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification (2 Methods)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Baseline:&lt;/strong&gt; Scaled Euclidean distance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KNN (k=4):&lt;/strong&gt; Chosen label from majority of 4 nearest neighbors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confusion Matrix Evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Achieved 93‚Äì100% accuracy on 5 classes&lt;/li&gt;
&lt;li&gt;Evaluated with multiple images per class under varied conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-extended-features&#34;&gt;üîß Extended Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recognizes &lt;strong&gt;11 total object classes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Handles &lt;strong&gt;multiple objects&lt;/strong&gt; simultaneously in a frame&lt;/li&gt;
&lt;li&gt;Classifies objects even under &lt;strong&gt;rotations and scaling&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-demo-mode&#34;&gt;üîÅ Demo Mode&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;&#39;r&#39;&lt;/code&gt; to begin recording video&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;&#39;p&#39;&lt;/code&gt; to stop and save to MP4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;üé• &lt;a href=&#34;https://drive.google.com/file/d/1AFoqLXxugcIa1uGtxgGTe1So_fgyTtBC/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Live System Demo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üìÅ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/Project%203-Report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/real-time-object-classifier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project combines classic computer vision with real-time systems and demonstrates how simple shape-based features can provide surprisingly effective results when paired with robust classification logic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Camera Calibration and Augmented Reality</title>
      <link>http://localhost:1313/project/calibration/</link>
      <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/calibration/</guid>
      <description>&lt;p&gt;This project focused on calibrating a camera and using it to overlay &lt;strong&gt;virtual 3D objects in real-time video feeds&lt;/strong&gt; using OpenCV. It introduced concepts in camera intrinsics, pose estimation, and interactive augmented reality applications.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-project-highlights&#34;&gt;üìå Project Highlights&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Corner Detection &amp;amp; Calibration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detected checkerboard corners using OpenCV&lt;/li&gt;
&lt;li&gt;Saved 3D-2D point correspondences&lt;/li&gt;
&lt;li&gt;Performed calibration with &lt;code&gt;cv2.calibrateCamera()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Achieved reprojection error ‚âà 0.544&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pose Estimation &amp;amp; Projection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used &lt;code&gt;solvePnP()&lt;/code&gt; to compute pose&lt;/li&gt;
&lt;li&gt;Visualized 3D axes on the image using &lt;code&gt;projectPoints&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Render Virtual Objects&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designed a 15-point &amp;ldquo;C&amp;rdquo; shape object in world space&lt;/li&gt;
&lt;li&gt;Rendered its projection on camera view&lt;/li&gt;
&lt;li&gt;Preserved 3D orientation as the camera moved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-extra-features&#34;&gt;üß† Extra Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Harris corner detection to compare feature robustness&lt;/li&gt;
&lt;li&gt;Live insertion of objects via webcam&lt;/li&gt;
&lt;li&gt;Support for &lt;strong&gt;multiple checkerboards&lt;/strong&gt; in one scene&lt;/li&gt;
&lt;li&gt;Replaced the checkerboard region with a &lt;strong&gt;custom image overlay&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-extension-tasks&#34;&gt;üîç Extension Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Compared calibrations of internal vs external webcams&lt;/li&gt;
&lt;li&gt;Added keyboard control to insert virtual objects from pre-recorded videos&lt;/li&gt;
&lt;li&gt;Used different board sizes (9x6, 6x6) in the same frame&lt;/li&gt;
&lt;li&gt;Created &lt;strong&gt;multi-object AR scenes&lt;/strong&gt; from multiple markers&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üìÅ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/Project%204-Report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üé• &lt;a href=&#34;https://drive.google.com/file/d/103mUiSgL6q1pJ2LhHZq8z1r2gVORhIdA/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/augmented-reality-calibration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project was an exciting blend of &lt;strong&gt;computer vision theory and practical AR rendering&lt;/strong&gt; using OpenCV. It forms a solid foundation for advanced applications in robotics, pose tracking, and mixed-reality interfaces.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
