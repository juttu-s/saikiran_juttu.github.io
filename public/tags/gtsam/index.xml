<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GTSAM | Hugo Academic CV Theme</title>
    <link>http://localhost:1313/tags/gtsam/</link>
      <atom:link href="http://localhost:1313/tags/gtsam/index.xml" rel="self" type="application/rss+xml" />
    <description>GTSAM</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 07 Nov 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>GTSAM</title>
      <link>http://localhost:1313/tags/gtsam/</link>
    </image>
    
    <item>
      <title>Sparse 3D Reconstruction and Bundle Adjustment</title>
      <link>http://localhost:1313/project/sparse-reconstruction/</link>
      <pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/sparse-reconstruction/</guid>
      <description>&lt;p&gt;This project implements a full Structure from Motion (SfM) pipeline on a &lt;strong&gt;Buddha statue&lt;/strong&gt; using a sequence of &lt;strong&gt;24 grayscale images&lt;/strong&gt;. It combines feature detection, epipolar geometry, camera pose recovery, triangulation, and bundle adjustment.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;24 images of a wooden Buddha statue captured at different angles&lt;/li&gt;
&lt;li&gt;Enhanced using &lt;strong&gt;CLAHE (Contrast Limited Adaptive Histogram Equalization)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Features extracted using &lt;strong&gt;SIFT&lt;/strong&gt; with custom parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pipeline-overview&#34;&gt;Pipeline Overview&lt;/h3&gt;
&lt;h4 id=&#34;1-image-preprocessing&#34;&gt;1. Image Preprocessing&lt;/h4&gt;
&lt;p&gt;Using CLAHE improves contrast on low-texture surfaces like carved wood.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;img src=&#34;processed.png&#34; width=&#34;1000&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-sift-feature-detection&#34;&gt;2. SIFT Feature Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Applied to all 24 images&lt;/li&gt;
&lt;li&gt;Used &lt;strong&gt;BFMatcher&lt;/strong&gt; with ratio test&lt;/li&gt;
&lt;li&gt;Matches filtered via &lt;strong&gt;RANSAC&lt;/strong&gt; for outlier rejection&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;img src=&#34;features.png&#34; width=&#34;1000&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-essential-matrix--pose-recovery&#34;&gt;3. Essential Matrix &amp;amp; Pose Recovery&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Computed Essential matrix using calibrated camera matrix&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;cv2.recoverPose()&lt;/code&gt; to derive relative rotation and translation between views&lt;/li&gt;
&lt;li&gt;Built a chain of camera poses from image 0 onward&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-triangulation&#34;&gt;4. Triangulation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;3D points computed from pixel correspondences using &lt;code&gt;cv2.triangulatePoints()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All 3D points stored in homogeneous form&lt;/li&gt;
&lt;li&gt;Colored and visualized using Plotly&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;5-bundle-adjustment-with-gtsam&#34;&gt;5. Bundle Adjustment with GTSAM&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built a factor graph with:
&lt;ul&gt;
&lt;li&gt;Camera pose priors&lt;/li&gt;
&lt;li&gt;Between factors from pose transitions&lt;/li&gt;
&lt;li&gt;Projection factors from 2D-3D matches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;Levenberg-MarquardtOptimizer&lt;/code&gt; for refinement&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;h4 id=&#34;initial-3d-trajectory&#34;&gt;Initial 3D Trajectory&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Initial Trajectory&#34; srcset=&#34;
               /project/sparse-reconstruction/Initial_plot_hu12090084634136375331.webp 400w,
               /project/sparse-reconstruction/Initial_plot_hu6971695684859321629.webp 760w,
               /project/sparse-reconstruction/Initial_plot_hu11908849571792030370.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/sparse-reconstruction/Initial_plot_hu12090084634136375331.webp&#34;
               width=&#34;760&#34;
               height=&#34;282&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;optimized-3d-trajectory-after-bundle-adjustment&#34;&gt;Optimized 3D Trajectory after Bundle Adjustment&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Optimized Trajectory&#34; srcset=&#34;
               /project/sparse-reconstruction/Optimized_plot_hu1365288965832748826.webp 400w,
               /project/sparse-reconstruction/Optimized_plot_hu5432368712792554619.webp 760w,
               /project/sparse-reconstruction/Optimized_plot_hu6617160073620067270.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/sparse-reconstruction/Optimized_plot_hu1365288965832748826.webp&#34;
               width=&#34;760&#34;
               height=&#34;282&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average reprojection error reduced by ~15%&lt;/li&gt;
&lt;li&gt;Landmark cloud tightened around object geometry&lt;/li&gt;
&lt;li&gt;Rotation drift corrected with global optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;tools--libraries&#34;&gt;Tools &amp;amp; Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenCV (SIFT, RANSAC, triangulation)&lt;/li&gt;
&lt;li&gt;NumPy, Matplotlib, Plotly&lt;/li&gt;
&lt;li&gt;GTSAM (factor graph + BA)&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/Sparse-3D-Reconstruction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project demonstrates a scalable pipeline for SfM using minimal dependencies. It serves as a foundation for integrating real-time VIO or stereo SLAM on embedded platforms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Photo Mosaicking of Low-Contrast Underwater Images</title>
      <link>http://localhost:1313/project/photo-mosaicking/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/photo-mosaicking/</guid>
      <description>&lt;p&gt;This project implements a full photo mosaicking and optimization pipeline using low-contrast underwater images from the &lt;strong&gt;Skerki Bank Roman shipwreck&lt;/strong&gt; dataset. The approach registers both sequential and non-sequential images using SIFT and RANSAC, computes affine transformations, and optimizes a global trajectory using GTSAM.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pipeline-breakdown&#34;&gt;Pipeline Breakdown&lt;/h3&gt;
&lt;h4 id=&#34;1-clahe-image-enhancement&#34;&gt;1Ô∏è‚É£ CLAHE Image Enhancement&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Applies histogram equalization to improve contrast and enhance keypoints.&lt;/li&gt;
&lt;li&gt;OpenCV CLAHE was used on each grayscale frame.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clahe&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;createCLAHE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clipLimit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tileGridSize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clahe_image&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clahe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gray_image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/clahe_grid.png&#34; alt=&#34;CLAHE Enhanced Images&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-sift-feature-detection&#34;&gt;2Ô∏è‚É£ SIFT Feature Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Detected keypoints using tuned SIFT settings:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nfeatures=5000&lt;/code&gt;, &lt;code&gt;contrastThreshold=0.025&lt;/code&gt;, &lt;code&gt;nOctaveLayers=8&lt;/code&gt;, &lt;code&gt;sigma=1.5&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sift&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SIFT_create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;kp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sift&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;detectAndCompute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/features_grid.png&#34; alt=&#34;Detected Keypoints&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-feature-matching--ransac-filtering&#34;&gt;3Ô∏è‚É£ Feature Matching + RANSAC Filtering&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Matched descriptors using Brute-Force Matcher + Lowe‚Äôs ratio test.&lt;/li&gt;
&lt;li&gt;Applied &lt;code&gt;cv2.estimateAffine2D&lt;/code&gt; with RANSAC to compute and refine transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;knnMatch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;des1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;des2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;good&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.75&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;estimateAffine2D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;method&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RANSAC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/affine_grid.png&#34; alt=&#34;Affine Transformation Filtering&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-pose-graph-construction-gtsam&#34;&gt;4Ô∏è‚É£ Pose Graph Construction (GTSAM)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built a factor graph using all non-repeating image pairs.&lt;/li&gt;
&lt;li&gt;Relative poses (affine transforms) were added as edges.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BetweenFactorPose2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T_ij&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;noise_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;initial-trajectoryplot_beforepng&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Initial Trajectory&#34; srcset=&#34;
               /project/photo-mosaicking/plot_before_hu6875028771095386571.webp 400w,
               /project/photo-mosaicking/plot_before_hu3546410195309766098.webp 760w,
               /project/photo-mosaicking/plot_before_hu3825931163024726418.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/photo-mosaicking/plot_before_hu6875028771095386571.webp&#34;
               width=&#34;571&#34;
               height=&#34;455&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/h2&gt;
&lt;h4 id=&#34;5-global-bundle-adjustment&#34;&gt;5Ô∏è‚É£ Global Bundle Adjustment&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Used GTSAM‚Äôs Levenberg-Marquardt optimizer to refine global poses.&lt;/li&gt;
&lt;li&gt;Corrects drift and adjusts poses to minimize total residual error.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gtsam&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LevenbergMarquardtOptimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_estimate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Optimized Trajectory&#34; srcset=&#34;
               /project/photo-mosaicking/plot_after_hu16779151673749428898.webp 400w,
               /project/photo-mosaicking/plot_after_hu14280678253068865284.webp 760w,
               /project/photo-mosaicking/plot_after_hu13790384701387224375.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/photo-mosaicking/plot_after_hu16779151673749428898.webp&#34;
               width=&#34;580&#34;
               height=&#34;455&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;techniques-used&#34;&gt;Techniques Used&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Image normalization + CLAHE&lt;/li&gt;
&lt;li&gt;SIFT feature detection and matching&lt;/li&gt;
&lt;li&gt;RANSAC for outlier rejection&lt;/li&gt;
&lt;li&gt;Homography estimation using Levenberg‚ÄìMarquardt&lt;/li&gt;
&lt;li&gt;Graph construction (GTSAM)&lt;/li&gt;
&lt;li&gt;Loop closure detection&lt;/li&gt;
&lt;li&gt;Pose optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Successfully registered both sequential and non-sequential image pairs&lt;/li&gt;
&lt;li&gt;Constructed optimized pose graphs for 6 and 29 image subsets&lt;/li&gt;
&lt;li&gt;Achieved a ~20% improvement in alignment after bundle adjustment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-related-files&#34;&gt;üìÅ Related Files&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/photo-mosaicking-skerki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìÅ &lt;a href=&#34;https://drive.google.com/drive/folders/1AtvT65txGIgAG23NRs3EkvDET036a81O&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Skerki Dataset Reference (Google Drive)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìì &lt;a href=&#34;http://localhost:1313/files/Part1_and_2.ipynb&#34;&gt;Project Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìì &lt;a href=&#34;http://localhost:1313/files/Part_3.ipynb&#34;&gt;Extended Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-references&#34;&gt;üìñ References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pizarro &amp;amp; Singh (2003): &lt;em&gt;Toward large-area mosaicing for underwater scientific applications.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ballard et al. (1998, 2000): &lt;em&gt;Roman shipwreck discovery using submersible tech.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Kimera VIO on EuRoC and Custom Datasets</title>
      <link>http://localhost:1313/project/kimera-vio/</link>
      <pubDate>Fri, 26 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/kimera-vio/</guid>
      <description>&lt;p&gt;This project evaluates the &lt;strong&gt;Kimera Visual-Inertial Odometry (VIO)&lt;/strong&gt; framework on two datasets ‚Äî the &lt;strong&gt;EuRoC MAV benchmark&lt;/strong&gt; and a &lt;strong&gt;custom indoor dataset&lt;/strong&gt; ‚Äî to assess its accuracy, runtime efficiency, and adaptability.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-project-goals&#34;&gt;üß† Project Goals&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Set up Kimera VIO in a ROS + Docker environment&lt;/li&gt;
&lt;li&gt;Run it on benchmark and custom datasets&lt;/li&gt;
&lt;li&gt;Evaluate using ATE RMSE, runtime, and qualitative analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-methodology&#34;&gt;üîç Methodology&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;VIO Front-End:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature detection (Shi-Tomasi)&lt;/li&gt;
&lt;li&gt;Optical flow tracking (Lucas-Kanade)&lt;/li&gt;
&lt;li&gt;Depth estimation via stereo matching&lt;/li&gt;
&lt;li&gt;Outlier filtering for robust constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;VIO Back-End:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Factor graph construction (GTSAM)&lt;/li&gt;
&lt;li&gt;iSAM2 for incremental optimization&lt;/li&gt;
&lt;li&gt;Pose, velocity, and landmark estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-evaluation&#34;&gt;üß™ Evaluation&lt;/h3&gt;
&lt;h4 id=&#34;-dataset-1-euroc-mav&#34;&gt;‚úÖ Dataset 1: EuRoC MAV&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ran Kimera VIO on V1_01 to V1_03&lt;/li&gt;
&lt;li&gt;Compared RMSE with VINS-Mono, OKVIS, MSCKF&lt;/li&gt;
&lt;li&gt;Achieved best accuracy across all sequences&lt;/li&gt;
&lt;li&gt;Average RMSE: 0.05‚Äì0.08&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;-dataset-2-custom-d455-dataset&#34;&gt;üß™ Dataset 2: Custom D455 Dataset&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Real-world indoor environment&lt;/li&gt;
&lt;li&gt;Feature-sparse and dynamically lit scenes&lt;/li&gt;
&lt;li&gt;Maintained smooth and consistent trajectory&lt;/li&gt;
&lt;li&gt;Minor drift in Z axis corrected by loop closures&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-tools--libraries&#34;&gt;‚öôÔ∏è Tools &amp;amp; Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kimera VIO&lt;/strong&gt; from MIT-SPARK Lab&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GTSAM&lt;/strong&gt; for factor graph optimization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open3D&lt;/strong&gt;, &lt;strong&gt;Matplotlib&lt;/strong&gt; for visualization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intel Realsense D455&lt;/strong&gt;, ROS, Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üßæ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/MR_Project_Report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/MIT-SPARK/Kimera-VIO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kimera VIO GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Kimera VIO proved to be a robust, modular SLAM framework capable of producing accurate trajectories under both structured (EuRoC) and challenging (custom indoor) conditions. Loop closure and factor graph smoothing allowed it to adapt well even with limited visual features.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
