<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Hugo Academic CV Theme</title>
    <link>http://localhost:1313/project/</link>
      <atom:link href="http://localhost:1313/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>Projects</title>
      <link>http://localhost:1313/project/</link>
    </image>
    
    <item>
      <title>Autonomous Race Driving with Action Mapping RL</title>
      <link>http://localhost:1313/project/am-race-driving-rl/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/am-race-driving-rl/</guid>
      <description>&lt;h2 id=&#34;-autonomous-race-driving-with-action-mapping-rl&#34;&gt;üèéÔ∏è Autonomous Race Driving with Action Mapping RL&lt;/h2&gt;
&lt;p&gt;This project evaluates the impact of &lt;strong&gt;Action Mapping (AM)&lt;/strong&gt; on &lt;strong&gt;deep reinforcement learning (DRL)&lt;/strong&gt; algorithms for high-speed autonomous race driving. The primary goal was to enforce &lt;strong&gt;friction and vehicle dynamics constraints&lt;/strong&gt; while ensuring agents could learn to complete laps efficiently and safely.&lt;/p&gt;
&lt;p&gt;We &lt;strong&gt;extended the GitHub framework from the AM-RL paper&lt;/strong&gt; (ISA Transactions 2024) that used TD3 + AM, and &lt;strong&gt;re-implemented the actor-critic models using DDPG and SAC&lt;/strong&gt; with Action Mapping. The experiments were conducted on &lt;strong&gt;Track A&lt;/strong&gt; in the custom race simulator.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Understand and replicate the Action Mapping Reinforcement Learning (AM-RL) framework&lt;/li&gt;
&lt;li&gt;Implement DDPG and SAC as alternate agents within the same setup&lt;/li&gt;
&lt;li&gt;Compare lap time, constraint satisfaction, and learning stability across methods&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;simulation--state-design&#34;&gt;Simulation &amp;amp; State Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Environment:&lt;/strong&gt; Custom Python-based simulator using bicycle model dynamics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observations:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;12 future curvature points from the centerline&lt;/li&gt;
&lt;li&gt;Velocity, yaw rate, orientation, steering angle, and track deviation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action Space:&lt;/strong&gt; Virtual actions ‚àà [‚àí1, 1]¬≤ ‚Üí throttle &amp;amp; steering angle&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Constraints:&lt;/strong&gt; Friction cone and lateral acceleration safety bounds&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neural-networks&#34;&gt;Neural Networks&lt;/h2&gt;
&lt;h3 id=&#34;actor-networks&#34;&gt;Actor Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fully connected with 2 hidden layers (256‚Äì256 units)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activation:&lt;/strong&gt; ReLU (hidden), Tanh (output)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; Unconstrained actions (steering, throttle) passed to AM module&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;critic-networks&#34;&gt;Critic Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Two Q-functions for SAC (double critic)&lt;/li&gt;
&lt;li&gt;One Q-function for DDPG&lt;/li&gt;
&lt;li&gt;Each accepts state-action pair and outputs scalar Q-value&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DDPG:&lt;/strong&gt; MSE loss on predicted Q-values&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAC:&lt;/strong&gt; Adds entropy regularization and automatic temperature tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;action-mapping-am&#34;&gt;Action Mapping (AM)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;State-aware mapping transforms virtual actions to safe, real-world controls&lt;/li&gt;
&lt;li&gt;Implemented as a lookup-based mapping based on track geometry and physics&lt;/li&gt;
&lt;li&gt;Ensures compliance with dynamic tire-road constraints (F‚Çì, F·µß within friction ellipse)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results-summary&#34;&gt;Results Summary&lt;/h2&gt;
&lt;h3 id=&#34;ddpg-with-action-mapping-am&#34;&gt;DDPG with Action Mapping (AM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Learned a stable and smooth racing policy&lt;/li&gt;
&lt;li&gt;Consistently finished laps on Track A&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lap 1 Time:&lt;/strong&gt; 43.57s&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lap 2 Time:&lt;/strong&gt; 41.7s&lt;/li&gt;
&lt;li&gt;Maintained safety constraints with no friction violations&lt;/li&gt;
&lt;li&gt;Approached TD3-level performance from literature&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;td3-vs-ddpg-lap-comparison&#34;&gt;TD3 vs DDPG Lap Comparison&lt;/h3&gt;
&lt;p&gt;Below is a side-by-side comparison of &lt;strong&gt;TD3&lt;/strong&gt; (left) and &lt;strong&gt;DDPG&lt;/strong&gt; (right) for both Lap 1 and Lap 2 on Track A. Color encodes speed (m/s).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;TD3 vs DDPG Lap 1 &amp;amp; 2&#34; srcset=&#34;
               /project/am-race-driving-rl/TD3_DDPG_hu11255701520106207767.webp 400w,
               /project/am-race-driving-rl/TD3_DDPG_hu16877567874459994854.webp 760w,
               /project/am-race-driving-rl/TD3_DDPG_hu9699183390352886908.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/am-race-driving-rl/TD3_DDPG_hu11255701520106207767.webp&#34;
               width=&#34;601&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure: Lap 1 and Lap 2 performance comparison ‚Äî TD3 has tighter trajectories and better cornering speed than DDPG.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sac-with-action-mapping-am&#34;&gt;SAC with Action Mapping (AM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Frequently failed to complete laps (DNF)&lt;/li&gt;
&lt;li&gt;Produced two common failure modes:
&lt;ul&gt;
&lt;li&gt;Prioritizing speed ‚Üí unstable cornering and off-track behavior&lt;/li&gt;
&lt;li&gt;Prioritizing stability ‚Üí very slow but accurate turns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Final policies were inconsistent and fragile&lt;/li&gt;
&lt;li&gt;Highly sensitive to reward scaling and actor update frequency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;SAC Lap 1 Intermediate Failures&#34; srcset=&#34;
               /project/am-race-driving-rl/SAC_hu7060679015506459835.webp 400w,
               /project/am-race-driving-rl/SAC_hu17451594598186353989.webp 760w,
               /project/am-race-driving-rl/SAC_hu100650002741892404.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/am-race-driving-rl/SAC_hu7060679015506459835.webp&#34;
               width=&#34;655&#34;
               height=&#34;377&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure: SAC Lap 1 ‚Äî Left: high speed, low stability; Right: better track alignment, very low velocity.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;key-comparison-table&#34;&gt;Key Comparison Table&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Metric&lt;/th&gt;
          &lt;th&gt;TD3-AM (paper)&lt;/th&gt;
          &lt;th&gt;DDPG-AM (ours)&lt;/th&gt;
          &lt;th&gt;SAC-AM (ours)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Best Lap Time (s)&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;36.94&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;41.7&lt;/td&gt;
          &lt;td&gt;DNF (unstable)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Constraint Violations&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;Some (off-track)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Convergence Speed&lt;/td&gt;
          &lt;td&gt;Fast&lt;/td&gt;
          &lt;td&gt;Moderate&lt;/td&gt;
          &lt;td&gt;Slow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Stability&lt;/td&gt;
          &lt;td&gt;High&lt;/td&gt;
          &lt;td&gt;High&lt;/td&gt;
          &lt;td&gt;Very Low&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;CS5180___Final_Report.pdf&#34;&gt;Final Report PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0019057824002143?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Original Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/s-bray/CS5180-Final_Project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project demonstrates how &lt;strong&gt;Action Mapping serves as a universal safety wrapper&lt;/strong&gt; that can be applied across different DRL algorithms. Our experiments showed that DDPG performs competitively against TD3 in constrained driving, while SAC struggled to converge due to reward instability ‚Äî emphasizing the importance of choosing the right policy architecture for safety-critical RL tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse 3D Reconstruction and Bundle Adjustment</title>
      <link>http://localhost:1313/project/sparse-reconstruction/</link>
      <pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/sparse-reconstruction/</guid>
      <description>&lt;p&gt;This project implements a full Structure from Motion (SfM) pipeline on a &lt;strong&gt;Buddha statue&lt;/strong&gt; using a sequence of &lt;strong&gt;24 grayscale images&lt;/strong&gt;. It combines feature detection, epipolar geometry, camera pose recovery, triangulation, and bundle adjustment.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;24 images of a wooden Buddha statue captured at different angles&lt;/li&gt;
&lt;li&gt;Enhanced using &lt;strong&gt;CLAHE (Contrast Limited Adaptive Histogram Equalization)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Features extracted using &lt;strong&gt;SIFT&lt;/strong&gt; with custom parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pipeline-overview&#34;&gt;Pipeline Overview&lt;/h3&gt;
&lt;h4 id=&#34;1-image-preprocessing&#34;&gt;1. Image Preprocessing&lt;/h4&gt;
&lt;p&gt;Using CLAHE improves contrast on low-texture surfaces like carved wood.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;img src=&#34;processed.png&#34; width=&#34;1000&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-sift-feature-detection&#34;&gt;2. SIFT Feature Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Applied to all 24 images&lt;/li&gt;
&lt;li&gt;Used &lt;strong&gt;BFMatcher&lt;/strong&gt; with ratio test&lt;/li&gt;
&lt;li&gt;Matches filtered via &lt;strong&gt;RANSAC&lt;/strong&gt; for outlier rejection&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;img src=&#34;features.png&#34; width=&#34;1000&#34;&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-essential-matrix--pose-recovery&#34;&gt;3. Essential Matrix &amp;amp; Pose Recovery&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Computed Essential matrix using calibrated camera matrix&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;cv2.recoverPose()&lt;/code&gt; to derive relative rotation and translation between views&lt;/li&gt;
&lt;li&gt;Built a chain of camera poses from image 0 onward&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-triangulation&#34;&gt;4. Triangulation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;3D points computed from pixel correspondences using &lt;code&gt;cv2.triangulatePoints()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All 3D points stored in homogeneous form&lt;/li&gt;
&lt;li&gt;Colored and visualized using Plotly&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;5-bundle-adjustment-with-gtsam&#34;&gt;5. Bundle Adjustment with GTSAM&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built a factor graph with:
&lt;ul&gt;
&lt;li&gt;Camera pose priors&lt;/li&gt;
&lt;li&gt;Between factors from pose transitions&lt;/li&gt;
&lt;li&gt;Projection factors from 2D-3D matches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Used &lt;code&gt;Levenberg-MarquardtOptimizer&lt;/code&gt; for refinement&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;h4 id=&#34;initial-3d-trajectory&#34;&gt;Initial 3D Trajectory&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Initial Trajectory&#34; srcset=&#34;
               /project/sparse-reconstruction/Initial_plot_hu12090084634136375331.webp 400w,
               /project/sparse-reconstruction/Initial_plot_hu6971695684859321629.webp 760w,
               /project/sparse-reconstruction/Initial_plot_hu11908849571792030370.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/sparse-reconstruction/Initial_plot_hu12090084634136375331.webp&#34;
               width=&#34;760&#34;
               height=&#34;282&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;optimized-3d-trajectory-after-bundle-adjustment&#34;&gt;Optimized 3D Trajectory after Bundle Adjustment&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Optimized Trajectory&#34; srcset=&#34;
               /project/sparse-reconstruction/Optimized_plot_hu1365288965832748826.webp 400w,
               /project/sparse-reconstruction/Optimized_plot_hu5432368712792554619.webp 760w,
               /project/sparse-reconstruction/Optimized_plot_hu6617160073620067270.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/sparse-reconstruction/Optimized_plot_hu1365288965832748826.webp&#34;
               width=&#34;760&#34;
               height=&#34;282&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average reprojection error reduced by ~15%&lt;/li&gt;
&lt;li&gt;Landmark cloud tightened around object geometry&lt;/li&gt;
&lt;li&gt;Rotation drift corrected with global optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;tools--libraries&#34;&gt;Tools &amp;amp; Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenCV (SIFT, RANSAC, triangulation)&lt;/li&gt;
&lt;li&gt;NumPy, Matplotlib, Plotly&lt;/li&gt;
&lt;li&gt;GTSAM (factor graph + BA)&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/Sparse-3D-Reconstruction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project demonstrates a scalable pipeline for SfM using minimal dependencies. It serves as a foundation for integrating real-time VIO or stereo SLAM on embedded platforms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Photo Mosaicking of Low-Contrast Underwater Images</title>
      <link>http://localhost:1313/project/photo-mosaicking/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/photo-mosaicking/</guid>
      <description>&lt;p&gt;This project implements a full photo mosaicking and optimization pipeline using low-contrast underwater images from the &lt;strong&gt;Skerki Bank Roman shipwreck&lt;/strong&gt; dataset. The approach registers both sequential and non-sequential images using SIFT and RANSAC, computes affine transformations, and optimizes a global trajectory using GTSAM.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pipeline-breakdown&#34;&gt;Pipeline Breakdown&lt;/h3&gt;
&lt;h4 id=&#34;1-clahe-image-enhancement&#34;&gt;1Ô∏è‚É£ CLAHE Image Enhancement&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Applies histogram equalization to improve contrast and enhance keypoints.&lt;/li&gt;
&lt;li&gt;OpenCV CLAHE was used on each grayscale frame.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clahe&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;createCLAHE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clipLimit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tileGridSize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clahe_image&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clahe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gray_image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/clahe_grid.png&#34; alt=&#34;CLAHE Enhanced Images&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-sift-feature-detection&#34;&gt;2Ô∏è‚É£ SIFT Feature Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Detected keypoints using tuned SIFT settings:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nfeatures=5000&lt;/code&gt;, &lt;code&gt;contrastThreshold=0.025&lt;/code&gt;, &lt;code&gt;nOctaveLayers=8&lt;/code&gt;, &lt;code&gt;sigma=1.5&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sift&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SIFT_create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;kp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sift&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;detectAndCompute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/features_grid.png&#34; alt=&#34;Detected Keypoints&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-feature-matching--ransac-filtering&#34;&gt;3Ô∏è‚É£ Feature Matching + RANSAC Filtering&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Matched descriptors using Brute-Force Matcher + Lowe‚Äôs ratio test.&lt;/li&gt;
&lt;li&gt;Applied &lt;code&gt;cv2.estimateAffine2D&lt;/code&gt; with RANSAC to compute and refine transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;knnMatch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;des1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;des2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;good&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.75&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;estimateAffine2D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;method&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RANSAC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/img/affine_grid.png&#34; alt=&#34;Affine Transformation Filtering&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;4-pose-graph-construction-gtsam&#34;&gt;4Ô∏è‚É£ Pose Graph Construction (GTSAM)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built a factor graph using all non-repeating image pairs.&lt;/li&gt;
&lt;li&gt;Relative poses (affine transforms) were added as edges.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BetweenFactorPose2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T_ij&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;noise_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;initial-trajectoryplot_beforepng&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Initial Trajectory&#34; srcset=&#34;
               /project/photo-mosaicking/plot_before_hu6875028771095386571.webp 400w,
               /project/photo-mosaicking/plot_before_hu3546410195309766098.webp 760w,
               /project/photo-mosaicking/plot_before_hu3825931163024726418.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/photo-mosaicking/plot_before_hu6875028771095386571.webp&#34;
               width=&#34;571&#34;
               height=&#34;455&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/h2&gt;
&lt;h4 id=&#34;5-global-bundle-adjustment&#34;&gt;5Ô∏è‚É£ Global Bundle Adjustment&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Used GTSAM‚Äôs Levenberg-Marquardt optimizer to refine global poses.&lt;/li&gt;
&lt;li&gt;Corrects drift and adjusts poses to minimize total residual error.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gtsam&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LevenbergMarquardtOptimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_estimate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Optimized Trajectory&#34; srcset=&#34;
               /project/photo-mosaicking/plot_after_hu16779151673749428898.webp 400w,
               /project/photo-mosaicking/plot_after_hu14280678253068865284.webp 760w,
               /project/photo-mosaicking/plot_after_hu13790384701387224375.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/photo-mosaicking/plot_after_hu16779151673749428898.webp&#34;
               width=&#34;580&#34;
               height=&#34;455&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;techniques-used&#34;&gt;Techniques Used&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Image normalization + CLAHE&lt;/li&gt;
&lt;li&gt;SIFT feature detection and matching&lt;/li&gt;
&lt;li&gt;RANSAC for outlier rejection&lt;/li&gt;
&lt;li&gt;Homography estimation using Levenberg‚ÄìMarquardt&lt;/li&gt;
&lt;li&gt;Graph construction (GTSAM)&lt;/li&gt;
&lt;li&gt;Loop closure detection&lt;/li&gt;
&lt;li&gt;Pose optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Successfully registered both sequential and non-sequential image pairs&lt;/li&gt;
&lt;li&gt;Constructed optimized pose graphs for 6 and 29 image subsets&lt;/li&gt;
&lt;li&gt;Achieved a ~20% improvement in alignment after bundle adjustment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-related-files&#34;&gt;üìÅ Related Files&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/photo-mosaicking-skerki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìÅ &lt;a href=&#34;https://drive.google.com/drive/folders/1AtvT65txGIgAG23NRs3EkvDET036a81O&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Skerki Dataset Reference (Google Drive)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìì &lt;a href=&#34;http://localhost:1313/files/Part1_and_2.ipynb&#34;&gt;Project Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üìì &lt;a href=&#34;http://localhost:1313/files/Part_3.ipynb&#34;&gt;Extended Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-references&#34;&gt;üìñ References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pizarro &amp;amp; Singh (2003): &lt;em&gt;Toward large-area mosaicing for underwater scientific applications.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ballard et al. (1998, 2000): &lt;em&gt;Roman shipwreck discovery using submersible tech.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Kimera VIO on EuRoC and Custom Datasets</title>
      <link>http://localhost:1313/project/kimera-vio/</link>
      <pubDate>Fri, 26 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/kimera-vio/</guid>
      <description>&lt;p&gt;This project evaluates the &lt;strong&gt;Kimera Visual-Inertial Odometry (VIO)&lt;/strong&gt; framework on two datasets ‚Äî the &lt;strong&gt;EuRoC MAV benchmark&lt;/strong&gt; and a &lt;strong&gt;custom indoor dataset&lt;/strong&gt; ‚Äî to assess its accuracy, runtime efficiency, and adaptability.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-project-goals&#34;&gt;üß† Project Goals&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Set up Kimera VIO in a ROS + Docker environment&lt;/li&gt;
&lt;li&gt;Run it on benchmark and custom datasets&lt;/li&gt;
&lt;li&gt;Evaluate using ATE RMSE, runtime, and qualitative analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-methodology&#34;&gt;üîç Methodology&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;VIO Front-End:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature detection (Shi-Tomasi)&lt;/li&gt;
&lt;li&gt;Optical flow tracking (Lucas-Kanade)&lt;/li&gt;
&lt;li&gt;Depth estimation via stereo matching&lt;/li&gt;
&lt;li&gt;Outlier filtering for robust constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;VIO Back-End:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Factor graph construction (GTSAM)&lt;/li&gt;
&lt;li&gt;iSAM2 for incremental optimization&lt;/li&gt;
&lt;li&gt;Pose, velocity, and landmark estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-evaluation&#34;&gt;üß™ Evaluation&lt;/h3&gt;
&lt;h4 id=&#34;-dataset-1-euroc-mav&#34;&gt;‚úÖ Dataset 1: EuRoC MAV&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ran Kimera VIO on V1_01 to V1_03&lt;/li&gt;
&lt;li&gt;Compared RMSE with VINS-Mono, OKVIS, MSCKF&lt;/li&gt;
&lt;li&gt;Achieved best accuracy across all sequences&lt;/li&gt;
&lt;li&gt;Average RMSE: 0.05‚Äì0.08&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;-dataset-2-custom-d455-dataset&#34;&gt;üß™ Dataset 2: Custom D455 Dataset&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Real-world indoor environment&lt;/li&gt;
&lt;li&gt;Feature-sparse and dynamically lit scenes&lt;/li&gt;
&lt;li&gt;Maintained smooth and consistent trajectory&lt;/li&gt;
&lt;li&gt;Minor drift in Z axis corrected by loop closures&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-tools--libraries&#34;&gt;‚öôÔ∏è Tools &amp;amp; Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kimera VIO&lt;/strong&gt; from MIT-SPARK Lab&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GTSAM&lt;/strong&gt; for factor graph optimization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open3D&lt;/strong&gt;, &lt;strong&gt;Matplotlib&lt;/strong&gt; for visualization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intel Realsense D455&lt;/strong&gt;, ROS, Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üßæ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/MR_Project_Report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/MIT-SPARK/Kimera-VIO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kimera VIO GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Kimera VIO proved to be a robust, modular SLAM framework capable of producing accurate trajectories under both structured (EuRoC) and challenging (custom indoor) conditions. Loop closure and factor graph smoothing allowed it to adapt well even with limited visual features.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated Insect Leg Labeling using DeepLabCut</title>
      <link>http://localhost:1313/project/automated-insect-leg-labeling/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/automated-insect-leg-labeling/</guid>
      <description>&lt;p&gt;This project proposes a robust and scalable method for automating insect leg labeling using &lt;strong&gt;image processing, feature detection, and clustering&lt;/strong&gt;, enabling seamless integration with &lt;strong&gt;DeepLabCut&lt;/strong&gt; for behavioral analysis.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;methodology-overview&#34;&gt;Methodology Overview&lt;/h2&gt;
&lt;h3 id=&#34;1-data-collection&#34;&gt;1. Data Collection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Captured insect locomotion data using a &lt;strong&gt;ServoSphere&lt;/strong&gt; robot equipped with omni-wheels and a high-speed camera.&lt;/li&gt;
&lt;li&gt;The camera tracked an insect (ant) placed atop the rotating sphere.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-preprocessing-pipeline&#34;&gt;2. Preprocessing Pipeline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Grayscale conversion&lt;/strong&gt; and &lt;strong&gt;Gaussian blur&lt;/strong&gt; for noise reduction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Canny Edge Detection&lt;/strong&gt; for edge enhancement&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binary thresholding&lt;/strong&gt; and &lt;strong&gt;morphological operations&lt;/strong&gt; for skeletonization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Canny Edge Result&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/canny_hu11275747406885085122.webp 400w,
               /project/automated-insect-leg-labeling/canny_hu15745542599272552929.webp 760w,
               /project/automated-insect-leg-labeling/canny_hu3473078471050911037.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/canny_hu11275747406885085122.webp&#34;
               width=&#34;555&#34;
               height=&#34;350&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Output after applying Canny edge detection.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-feature-extraction&#34;&gt;3. Feature Extraction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Applied &lt;strong&gt;Shi-Tomasi Corner Detection&lt;/strong&gt; on Canny edges for precise joint detection&lt;/li&gt;
&lt;li&gt;Robust to noise and sensitive to detailed motion features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Shi-Tomasi Detection&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/shi_tomasi_hu3655572418342755069.webp 400w,
               /project/automated-insect-leg-labeling/shi_tomasi_hu14103565545081466617.webp 760w,
               /project/automated-insect-leg-labeling/shi_tomasi_hu4890327641649225453.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/shi_tomasi_hu3655572418342755069.webp&#34;
               width=&#34;555&#34;
               height=&#34;350&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Shi-Tomasi corner detection highlights potential joint features.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-body-removal&#34;&gt;4. Body Removal&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Used &lt;strong&gt;Zhang-Suen thinning&lt;/strong&gt; for skeleton extraction&lt;/li&gt;
&lt;li&gt;Applied &lt;strong&gt;connected component labeling&lt;/strong&gt; to segment body parts&lt;/li&gt;
&lt;li&gt;Removed body via &lt;strong&gt;template matching&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Skeleton Extraction&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/skeleton_extraction_hu7723478630954022702.webp 400w,
               /project/automated-insect-leg-labeling/skeleton_extraction_hu1654188756838549732.webp 760w,
               /project/automated-insect-leg-labeling/skeleton_extraction_hu4799263873215453537.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/skeleton_extraction_hu7723478630954022702.webp&#34;
               width=&#34;553&#34;
               height=&#34;355&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Skeleton representation using Zhang-Suen thinning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Connected Components&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/connected_components_hu7217986797033856015.webp 400w,
               /project/automated-insect-leg-labeling/connected_components_hu12195037151038958192.webp 760w,
               /project/automated-insect-leg-labeling/connected_components_hu4918036970471170597.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/connected_components_hu7217986797033856015.webp&#34;
               width=&#34;545&#34;
               height=&#34;378&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Connected component labeling for body part isolation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Body Template&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/template_hu11659726123555705027.webp 400w,
               /project/automated-insect-leg-labeling/template_hu18209107862982516437.webp 760w,
               /project/automated-insect-leg-labeling/template_hu2884419823242467532.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/template_hu11659726123555705027.webp&#34;
               width=&#34;499&#34;
               height=&#34;353&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Template matching used to isolate and remove the body region.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-leg-detection&#34;&gt;5. Leg Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Remaining features correspond to legs&lt;/li&gt;
&lt;li&gt;Calculated angles of features w.r.t. body centroid&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Detected Leg Features&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/leg_features_hu7341448728120080587.webp 400w,
               /project/automated-insect-leg-labeling/leg_features_hu18101909398949920466.webp 760w,
               /project/automated-insect-leg-labeling/leg_features_hu14108854493791996505.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/leg_features_hu7341448728120080587.webp&#34;
               width=&#34;489&#34;
               height=&#34;320&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Extracted leg features post body removal.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Feature Angles&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/angles_hu911305775039293742.webp 400w,
               /project/automated-insect-leg-labeling/angles_hu16264059692635278634.webp 760w,
               /project/automated-insect-leg-labeling/angles_hu12389778951175088630.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/angles_hu911305775039293742.webp&#34;
               width=&#34;496&#34;
               height=&#34;338&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Angle estimation of each leg with respect to the centroid.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-clustering--tip-detection&#34;&gt;6. Clustering &amp;amp; Tip Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KMeans (K=6)&lt;/strong&gt; clusters features into 6 legs&lt;/li&gt;
&lt;li&gt;Furthest feature in each cluster = leg tip&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;KMeans Clustering&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/kmeans_hu2991777993052901984.webp 400w,
               /project/automated-insect-leg-labeling/kmeans_hu1537055730035741828.webp 760w,
               /project/automated-insect-leg-labeling/kmeans_hu8003083638116927705.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/kmeans_hu2991777993052901984.webp&#34;
               width=&#34;556&#34;
               height=&#34;406&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: KMeans clustering of features into 6 leg regions.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;7-integration-with-deeplabcut&#34;&gt;7. Integration with DeepLabCut&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Created &lt;code&gt;.h5&lt;/code&gt; files with clustered keypoints&lt;/li&gt;
&lt;li&gt;Trained DeepLabCut on auto-labeled dataset&lt;/li&gt;
&lt;li&gt;Achieved performance near manual labeling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;DLC Integration&#34; srcset=&#34;
               /project/automated-insect-leg-labeling/integration_hu13320621738235640093.webp 400w,
               /project/automated-insect-leg-labeling/integration_hu5511305675759056958.webp 760w,
               /project/automated-insect-leg-labeling/integration_hu10805202269737531730.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/automated-insect-leg-labeling/integration_hu13320621738235640093.webp&#34;
               width=&#34;671&#34;
               height=&#34;615&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure: Final annotated labels used with DeepLabCut.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Leg&lt;/th&gt;
          &lt;th&gt;TP (Auto)&lt;/th&gt;
          &lt;th&gt;FP (Auto)&lt;/th&gt;
          &lt;th&gt;TP (Manual)&lt;/th&gt;
          &lt;th&gt;FP (Manual)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;95&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;98&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;90&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;95&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;85&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;90&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;80&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;85&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;75&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;80&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;70&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;75&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Confusion matrix comparing auto-labeled vs manually labeled results. Accuracy slightly lower, but performance is consistent and scalable.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;key-techniques&#34;&gt;Key Techniques&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Canny Edge Detection&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shi-Tomasi GFTT&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Template Matching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KMeans Clustering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zhang-Suen Thinning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepLabCut integration&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;insights--future-work&#34;&gt;Insights &amp;amp; Future Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Automation significantly reduced manual effort&lt;/li&gt;
&lt;li&gt;High reproducibility across ant datasets&lt;/li&gt;
&lt;li&gt;KMeans produced sharper clusters than Ensemble KMeans&lt;/li&gt;
&lt;li&gt;Future work may explore deep learning-based leg segmentation and adaptive clustering strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/Pattern-Recognition-and-Computer-Vision/tree/main/Automated%20Insect%20Leg%20Labeling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This approach provides a &lt;strong&gt;generalizable and scalable method&lt;/strong&gt; for anatomical labeling in biological research and can be extended to other multi-limbed species or anatomical joints.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Digit Recognition &amp; Transfer Learning with CNNs</title>
      <link>http://localhost:1313/project/digit-recognition/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/digit-recognition/</guid>
      <description>&lt;p&gt;This project focused on &lt;strong&gt;designing, training, and analyzing convolutional neural networks (CNNs)&lt;/strong&gt; for digit recognition and then &lt;strong&gt;applying transfer learning&lt;/strong&gt; to recognize Greek characters using PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cnn-for-digit-recognition-mnist&#34;&gt;CNN for Digit Recognition (MNIST)&lt;/h2&gt;
&lt;p&gt;We first built and trained a simple CNN model using the MNIST dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two convolutional layers (5x5 filters)&lt;/li&gt;
&lt;li&gt;Max pooling (2x2) and dropout (0.5)&lt;/li&gt;
&lt;li&gt;Fully connected layer (50 nodes) + final FC with log-softmax (10 classes)&lt;/li&gt;
&lt;li&gt;Achieved &lt;strong&gt;~98% test accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Sample MNIST Digits&#34; srcset=&#34;
               /project/digit-recognition/firstsix_hu15902310905350978628.webp 400w,
               /project/digit-recognition/firstsix_hu10625707504812272563.webp 760w,
               /project/digit-recognition/firstsix_hu9923025005567100836.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/firstsix_hu15902310905350978628.webp&#34;
               width=&#34;708&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: Example of first 6 MNIST test samples.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;network-architecture&#34;&gt;Network Architecture&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CNN Diagram&#34; srcset=&#34;
               /project/digit-recognition/NN_hu7544495459113416032.webp 400w,
               /project/digit-recognition/NN_hu11458655653262536510.webp 760w,
               /project/digit-recognition/NN_hu5398614519963812698.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/NN_hu7544495459113416032.webp&#34;
               width=&#34;760&#34;
               height=&#34;509&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: End-to-end architecture used for digit recognition.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;training-performance&#34;&gt;Training Performance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Loss decreased steadily over batches/epochs.&lt;/li&gt;
&lt;li&gt;Test loss visualized periodically across training examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Loss Curve&#34; srcset=&#34;
               /project/digit-recognition/loss_hu2647141672864762400.webp 400w,
               /project/digit-recognition/loss_hu14298867483472384218.webp 760w,
               /project/digit-recognition/loss_hu3418306977125921203.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/loss_hu2647141672864762400.webp&#34;
               width=&#34;760&#34;
               height=&#34;559&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: Negative log-likelihood training and test loss.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;filter-visualization&#34;&gt;Filter Visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First conv layer has 10 learned filters.&lt;/li&gt;
&lt;li&gt;Visualized using matplotlib to show edge/orientation detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Filter Maps&#34; srcset=&#34;
               /project/digit-recognition/filter_maps1_hu6510234662074493762.webp 400w,
               /project/digit-recognition/filter_maps1_hu11416344688562476833.webp 760w,
               /project/digit-recognition/filter_maps1_hu16678790348298768722.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/filter_maps1_hu6510234662074493762.webp&#34;
               width=&#34;638&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: Learned 5x5 filters from first conv layer.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transfer-learning-to-greek-letters&#34;&gt;Transfer Learning to Greek Letters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fine-tuned MNIST CNN to classify Greek characters: Œ±, Œ≤, Œ≥.&lt;/li&gt;
&lt;li&gt;Froze earlier layers, retrained final classifier head.&lt;/li&gt;
&lt;li&gt;Achieved &lt;strong&gt;96‚Äì98% accuracy&lt;/strong&gt; on unseen test samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Greek Classification Output&#34; srcset=&#34;
               /project/digit-recognition/greek_hu4305634218149458193.webp 400w,
               /project/digit-recognition/greek_hu16620829909298196167.webp 760w,
               /project/digit-recognition/greek_hu10198682355568399909.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/greek_hu4305634218149458193.webp&#34;
               width=&#34;652&#34;
               height=&#34;391&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: Correct classification of Greek letters using transfer learning.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transfer-learning-training-error&#34;&gt;Transfer Learning Training Error&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convergence observed over 100+ epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Greek Training Error Curve&#34; srcset=&#34;
               /project/digit-recognition/greek_training_hu10070426372364170428.webp 400w,
               /project/digit-recognition/greek_training_hu13936645240799545862.webp 760w,
               /project/digit-recognition/greek_training_hu10173284984926363065.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/greek_training_hu10070426372364170428.webp&#34;
               width=&#34;640&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: Error vs epoch for Greek classification.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;live-webcam-digit-recognition&#34;&gt;Live Webcam Digit Recognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Built a webcam-based interface using OpenCV&lt;/li&gt;
&lt;li&gt;Live input frames classified in real time using the trained CNN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;live webcam&#34; srcset=&#34;
               /project/digit-recognition/live_hu16096523079256018570.webp 400w,
               /project/digit-recognition/live_hu4232723163901620511.webp 760w,
               /project/digit-recognition/live_hu8695550706911073452.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/digit-recognition/live_hu16096523079256018570.webp&#34;
               width=&#34;580&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;em&gt;Figure: Live Webcam Digit Recognition&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Sometimes predictions were inaccurate due to motion blur or poor lighting, but most digits were correctly classified in real-time.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;5x5 and 7x7 filters outperform 3x3 in MNIST CNNs&lt;/li&gt;
&lt;li&gt;Dropout around &lt;strong&gt;0.1‚Äì0.3&lt;/strong&gt; provided best generalization&lt;/li&gt;
&lt;li&gt;Smaller batch sizes (32) resulted in better accuracy&lt;/li&gt;
&lt;li&gt;Greek letters transfer learning worked well with small data (~27 samples)&lt;/li&gt;
&lt;li&gt;Real-time system demonstrates CNNs are efficient and deployable&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python, PyTorch, OpenCV&lt;/li&gt;
&lt;li&gt;Jupyter Notebooks&lt;/li&gt;
&lt;li&gt;Matplotlib for visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;Project.pdf&#34;&gt;Project Report PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/Pattern-Recognition-and-Computer-Vision/tree/main/Recognition%20using%20Deep%20Networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project deepened our understanding of CNN design, filter learning, transfer learning, and real-time deployment ‚Äî showing the full lifecycle from dataset to deployment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-Time 2D Object Recognition with Feature Matching</title>
      <link>http://localhost:1313/project/object-recognition/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/object-recognition/</guid>
      <description>&lt;p&gt;This project developed a &lt;strong&gt;real-time webcam-based system&lt;/strong&gt; to detect and classify 2D objects using image segmentation, shape features, and distance-based classifiers. The system supports capturing and labeling training data interactively and recognizes both scale- and rotation-invariant objects.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-workflow&#34;&gt;üß† Workflow&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Thresholding Input Frames&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic thresholding via simplified K-means clustering&lt;/li&gt;
&lt;li&gt;Binary image separates object from background&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Morphological Filtering&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used dilation and erosion to clean noise and fill holes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Connected Components &amp;amp; Region Segmentation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identified distinct objects&lt;/li&gt;
&lt;li&gt;Filtered out small noisy components&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Centroids, central moments, bounding box, percent fill&lt;/li&gt;
&lt;li&gt;Orientation angle from least central moment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Interface&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;&#39;n&#39;&lt;/code&gt; to label new objects and save features to CSV&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification (2 Methods)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Baseline:&lt;/strong&gt; Scaled Euclidean distance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KNN (k=4):&lt;/strong&gt; Chosen label from majority of 4 nearest neighbors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confusion Matrix Evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Achieved 93‚Äì100% accuracy on 5 classes&lt;/li&gt;
&lt;li&gt;Evaluated with multiple images per class under varied conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-extended-features&#34;&gt;üîß Extended Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recognizes &lt;strong&gt;11 total object classes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Handles &lt;strong&gt;multiple objects&lt;/strong&gt; simultaneously in a frame&lt;/li&gt;
&lt;li&gt;Classifies objects even under &lt;strong&gt;rotations and scaling&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-demo-mode&#34;&gt;üîÅ Demo Mode&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;&#39;r&#39;&lt;/code&gt; to begin recording video&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;&#39;p&#39;&lt;/code&gt; to stop and save to MP4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;üé• &lt;a href=&#34;https://drive.google.com/file/d/1AFoqLXxugcIa1uGtxgGTe1So_fgyTtBC/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Live System Demo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üìÅ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/Project%203-Report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/real-time-object-classifier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project combines classic computer vision with real-time systems and demonstrates how simple shape-based features can provide surprisingly effective results when paired with robust classification logic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Camera Calibration and Augmented Reality</title>
      <link>http://localhost:1313/project/calibration/</link>
      <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/calibration/</guid>
      <description>&lt;p&gt;This project focused on calibrating a camera and using it to overlay &lt;strong&gt;virtual 3D objects in real-time video feeds&lt;/strong&gt; using OpenCV. It introduced concepts in camera intrinsics, pose estimation, and interactive augmented reality applications.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-project-highlights&#34;&gt;üìå Project Highlights&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Corner Detection &amp;amp; Calibration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detected checkerboard corners using OpenCV&lt;/li&gt;
&lt;li&gt;Saved 3D-2D point correspondences&lt;/li&gt;
&lt;li&gt;Performed calibration with &lt;code&gt;cv2.calibrateCamera()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Achieved reprojection error ‚âà 0.544&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pose Estimation &amp;amp; Projection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used &lt;code&gt;solvePnP()&lt;/code&gt; to compute pose&lt;/li&gt;
&lt;li&gt;Visualized 3D axes on the image using &lt;code&gt;projectPoints&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Render Virtual Objects&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designed a 15-point &amp;ldquo;C&amp;rdquo; shape object in world space&lt;/li&gt;
&lt;li&gt;Rendered its projection on camera view&lt;/li&gt;
&lt;li&gt;Preserved 3D orientation as the camera moved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-extra-features&#34;&gt;üß† Extra Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Harris corner detection to compare feature robustness&lt;/li&gt;
&lt;li&gt;Live insertion of objects via webcam&lt;/li&gt;
&lt;li&gt;Support for &lt;strong&gt;multiple checkerboards&lt;/strong&gt; in one scene&lt;/li&gt;
&lt;li&gt;Replaced the checkerboard region with a &lt;strong&gt;custom image overlay&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-extension-tasks&#34;&gt;üîç Extension Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Compared calibrations of internal vs external webcams&lt;/li&gt;
&lt;li&gt;Added keyboard control to insert virtual objects from pre-recorded videos&lt;/li&gt;
&lt;li&gt;Used different board sizes (9x6, 6x6) in the same frame&lt;/li&gt;
&lt;li&gt;Created &lt;strong&gt;multi-object AR scenes&lt;/strong&gt; from multiple markers&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üìÅ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/Project%204-Report.pdf&#34;&gt;Project Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üé• &lt;a href=&#34;https://drive.google.com/file/d/103mUiSgL6q1pJ2LhHZq8z1r2gVORhIdA/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/augmented-reality-calibration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project was an exciting blend of &lt;strong&gt;computer vision theory and practical AR rendering&lt;/strong&gt; using OpenCV. It forms a solid foundation for advanced applications in robotics, pose tracking, and mixed-reality interfaces.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dead Reckoning with IMU &amp; GPS in Vehicle Navigation</title>
      <link>http://localhost:1313/project/dead-reckoning/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/dead-reckoning/</guid>
      <description>&lt;p&gt;This project involved real-world testing of &lt;strong&gt;dead reckoning navigation&lt;/strong&gt; using an &lt;strong&gt;IMU (VectorNav 100)&lt;/strong&gt; and GPS data mounted in a car. We collected and fused sensor data to estimate vehicle trajectory and compared it against ground truth GPS tracks.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-objective&#34;&gt;üìç Objective&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use IMU (accel, gyro, mag) for estimating yaw and velocity&lt;/li&gt;
&lt;li&gt;Compare integrated dead reckoning trajectory with GPS&lt;/li&gt;
&lt;li&gt;Apply calibration and filtering for high-speed vehicle navigation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-methodology&#34;&gt;üß™ Methodology&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sensor Mounting &amp;amp; Data Collection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IMU mounted flat, aligned to vehicle axis&lt;/li&gt;
&lt;li&gt;GPS puck on roof&lt;/li&gt;
&lt;li&gt;ROS bagged data over a 2‚Äì3 km drive around Boston&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Magnetometer Calibration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applied hard &amp;amp; soft iron corrections using ellipse fitting&lt;/li&gt;
&lt;li&gt;Centered &amp;amp; scaled magnetometer output&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Yaw Estimation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used complementary filter combining:
&lt;ul&gt;
&lt;li&gt;Low-pass filtered magnetometer (0.5 Hz cutoff)&lt;/li&gt;
&lt;li&gt;High-pass filtered gyro data (0.1 Hz cutoff)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fusion coefficient Œ± = 0.98&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Velocity Estimation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Acceleration filtered with high-pass Butterworth&lt;/li&gt;
&lt;li&gt;Integrated over time&lt;/li&gt;
&lt;li&gt;Clamped negative values to zero and capped outliers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trajectory Reconstruction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrated filtered heading + velocity&lt;/li&gt;
&lt;li&gt;Rotated &amp;amp; scaled to align IMU trajectory with GPS path&lt;/li&gt;
&lt;li&gt;Best match scale: 2.25, rotation correction: 14‚Äì25¬∞&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-results&#34;&gt;üìä Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Initial path tracking was highly accurate&lt;/li&gt;
&lt;li&gt;IMU-based dead reckoning remained within &lt;strong&gt;2m&lt;/strong&gt; of GPS for &lt;strong&gt;150‚Äì200 seconds&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Final drift reached nearly &lt;strong&gt;400 meters&lt;/strong&gt; after extended driving&lt;/li&gt;
&lt;li&gt;GPS helps correct IMU drift over long durations&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-key-insights&#34;&gt;üîé Key Insights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IMU is accurate for short-term dead reckoning&lt;/li&gt;
&lt;li&gt;Drift arises from gyroscope bias and sloped paths&lt;/li&gt;
&lt;li&gt;Regular GPS re-calibration is essential for long-term navigation&lt;/li&gt;
&lt;li&gt;Complementary filter effectively combines noisy sensors&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-resources&#34;&gt;üìÅ Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üìÑ &lt;a href=&#34;http://localhost:1313/files/LAB%205%20report-1.pdf&#34;&gt;Lab 5 Report (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://github.com/juttu-s/dead-reckoning-imu-gps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This project demonstrates real-world dead reckoning with consumer-grade sensors and sets the foundation for more robust SLAM and localization strategies in mobile robotics or autonomous vehicles.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
